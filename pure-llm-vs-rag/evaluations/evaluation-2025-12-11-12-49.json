{
  "Why is my pod stuck?": {
    "gpt-5-nano_base": {
      "input": {
        "question": "Why is my pod stuck?",
        "prompt": "You are a Kubernetes expert. Be concise and accurate.\n\nWhy is my pod stuck?",
        "system_prompt": "You are a Kubernetes expert. Be concise and accurate.",
        "human_prompt": "Why is my pod stuck?"
      },
      "response": "\u201cStuck\u201d depends on the pod\u2019s Status. Run these first and share the key lines from Events:\n- kubectl get pod POD -n NS -o wide\n- kubectl describe pod POD -n NS\n- kubectl get events -n NS --sort-by=.lastTimestamp | tail -n 30\n- If it restarts: kubectl logs POD -c CONTAINER --previous\n- If there are init containers: kubectl logs POD -c INIT_CONTAINER\n\nWhat to look for and how to fix:\n\n- Status: Pending\n  - Scheduler says \u201c0/NN nodes available\u201d with reasons:\n    - Insufficient cpu/memory/ephemeral-storage: lower requests, add nodes, or move workload.\n    - Node(s) had taint \u2026: add tolerations or remove taint.\n    - Affinity/nodeSelector/topologySpread constraints unsatisfiable: relax rules.\n  - PVC Pending: kubectl describe pvc; ensure StorageClass exists, quota not exceeded, PV available with matching access modes/size.\n\n- Status: ContainerCreating\n  - Image pull errors \u2192 ImagePullBackOff/ErrImagePull: fix image name/tag, registry reachability, imagePullSecrets.\n  - MountVolume.* failed: missing Secret/ConfigMap/CSI; verify names, namespace, keys; check CSI driver pods.\n  - \u201cFailed to create pod sandbox / CNI not initialized\u201d: check CNI plugin pods (kubectl get pods -n kube-system), node network.\n\n- Status: CreateContainerConfigError\n  - Bad envFrom/volume refs or missing keys: verify ConfigMap/Secret names/keys and volumeMount paths.\n\n- Status: CrashLoopBackOff\n  - App exits quickly or probes kill it. Check logs (use --previous). Fix command/args/config, or adjust liveness/readiness (initialDelaySeconds, timeouts). If OOMKilled, reduce memory usage or raise limits.\n\n- Status: Running but NotReady\n  - Readiness probe failing. Describe shows probe errors; fix endpoint/path/port/TLS, or increase timeouts.\n\n- Status: ImagePullBackOff / ErrImagePull\n  - Validate image, registry auth (imagePullSecrets), network/egress, tag existence.\n\n- Status: Terminating (stuck)\n  - Long preStop or app not handling SIGTERM; terminationGracePeriodSeconds too high: fix app or lower grace period.\n  - Finalizers/volume detach stuck: check finalizers (kubectl get pod POD -n NS -o json | jq .metadata.finalizers). As last resort: kubectl delete pod POD -n NS --force --grace-period=0.\n\n- Status: Unknown\n  - Node unreachable/notready. Check nodes and taints: kubectl get nodes; kubectl describe node NODE. Investigate node/kubelet/CNI.\n\nIf you paste:\n- Pod name/namespace\n- kubectl get pod ... output (STATUS, RESTARTS, NODE)\n- The Events section from kubectl describe\nI can pinpoint the exact cause and fix."
    },
    "gpt-5-nano_rag": {
      "input": {
        "question": "Why is my pod stuck?",
        "context": "kubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.\n\npod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nCheck the Service:\n\nkubectl get svc -l app.kubernetes.io/name=MyApp\n\nValidate that the Service receives a CLUSTER-IP address from the IPv6 address block along with an EXTERNAL-IP. You may then validate access to the service via the IP and port.\n\nNAME         TYPE           CLUSTER-IP            EXTERNAL-IP        PORT(S)        AGE\nmy-service   LoadBalancer   2001:db8:fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nThe kubelet manages the hosts file for each container of the Pod to prevent the container runtime from modifying the file after the containers have already been started. Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would then modify the /etc/hosts file after each container had started.\n\nCurrent Kubernetes can use a variety of container runtimes; even so, the kubelet manages the hosts file within each container so that the outcome is as intended regardless of which container runtime you use.\n\n{{< caution >}} Avoid making manual changes to the hosts file inside a container.\n\nIf you make manual changes to the hosts file, those changes are lost when the container exits. {{< /caution >}}",
        "context_docs": [
          {
            "content": "kubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.",
            "metadata": {
              "source": "k8s-docs/content/en/docs/tasks/network/validate-dual-stack.md",
              "_id": "13294645-0c19-4664-a72a-7a50d278db75",
              "_collection_name": "k8s_docs"
            }
          },
          {
            "content": "pod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}",
            "metadata": {
              "source": "k8s-docs/content/en/docs/tasks/network/customize-hosts-file-for-pods.md",
              "_id": "571da25e-1e70-41ed-aabc-f6517fd38d9e",
              "_collection_name": "k8s_docs"
            }
          },
          {
            "content": "Check the Service:\n\nkubectl get svc -l app.kubernetes.io/name=MyApp\n\nValidate that the Service receives a CLUSTER-IP address from the IPv6 address block along with an EXTERNAL-IP. You may then validate access to the service via the IP and port.\n\nNAME         TYPE           CLUSTER-IP            EXTERNAL-IP        PORT(S)        AGE\nmy-service   LoadBalancer   2001:db8:fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s",
            "metadata": {
              "source": "k8s-docs/content/en/docs/tasks/network/validate-dual-stack.md",
              "_id": "5fd0e97b-3f74-4bdf-b2c2-f6a251143eb7",
              "_collection_name": "k8s_docs"
            }
          },
          {
            "content": "Why does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nThe kubelet manages the hosts file for each container of the Pod to prevent the container runtime from modifying the file after the containers have already been started. Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would then modify the /etc/hosts file after each container had started.\n\nCurrent Kubernetes can use a variety of container runtimes; even so, the kubelet manages the hosts file within each container so that the outcome is as intended regardless of which container runtime you use.\n\n{{< caution >}} Avoid making manual changes to the hosts file inside a container.\n\nIf you make manual changes to the hosts file, those changes are lost when the container exits. {{< /caution >}}",
            "metadata": {
              "source": "k8s-docs/content/en/docs/tasks/network/customize-hosts-file-for-pods.md",
              "_id": "215f4cbf-3893-451a-b060-6c80fda64336",
              "_collection_name": "k8s_docs"
            }
          }
        ],
        "prompt": "You are a Kubernetes expert. Use this context:\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.\n\npod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nCheck the Service:\n\nkubectl get svc -l app.kubernetes.io/name=MyApp\n\nValidate that the Service receives a CLUSTER-IP address from the IPv6 address block along with an EXTERNAL-IP. You may then validate access to the service via the IP and port.\n\nNAME         TYPE           CLUSTER-IP            EXTERNAL-IP        PORT(S)        AGE\nmy-service   LoadBalancer   2001:db8:fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nThe kubelet manages the hosts file for each container of the Pod to prevent the container runtime from modifying the file after the containers have already been started. Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would then modify the /etc/hosts file after each container had started.\n\nCurrent Kubernetes can use a variety of container runtimes; even so, the kubelet manages the hosts file within each container so that the outcome is as intended regardless of which container runtime you use.\n\n{{< caution >}} Avoid making manual changes to the hosts file inside a container.\n\nIf you make manual changes to the hosts file, those changes are lost when the container exits. {{< /caution >}}\n\nWhy is my pod stuck?",
        "system_prompt": "You are a Kubernetes expert. Use this context:\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.\n\npod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nCheck the Service:\n\nkubectl get svc -l app.kubernetes.io/name=MyApp\n\nValidate that the Service receives a CLUSTER-IP address from the IPv6 address block along with an EXTERNAL-IP. You may then validate access to the service via the IP and port.\n\nNAME         TYPE           CLUSTER-IP            EXTERNAL-IP        PORT(S)        AGE\nmy-service   LoadBalancer   2001:db8:fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nThe kubelet manages the hosts file for each container of the Pod to prevent the container runtime from modifying the file after the containers have already been started. Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would then modify the /etc/hosts file after each container had started.\n\nCurrent Kubernetes can use a variety of container runtimes; even so, the kubelet manages the hosts file within each container so that the outcome is as intended regardless of which container runtime you use.\n\n{{< caution >}} Avoid making manual changes to the hosts file inside a container.\n\nIf you make manual changes to the hosts file, those changes are lost when the container exits. {{< /caution >}}",
        "human_prompt": "Why is my pod stuck?"
      },
      "response": "\u201cStuck\u201d can mean different things in Kubernetes. Help me narrow it down by sharing:\n- The namespace and pod name\n- Output of: kubectl get pod <pod> -n <ns> -o wide\n- Output of: kubectl describe pod <pod> -n <ns>\n- Logs: kubectl logs <pod> -n <ns> --all-containers --previous\n\nMeanwhile, here\u2019s how to quickly diagnose the most common \u201cstuck\u201d situations and what to do:\n\n1) Pod shows 0/1 Completed\n- Meaning: Your container exited successfully; the Pod finished. This is normal for Jobs or Pods that run once and exit.\n- Fix if you expected it to keep running:\n  - Use a long-running command (eg: sleep infinity or tail -f /dev/null).\n  - For Deployments, ensure restartPolicy: Always (Jobs typically use OnFailure/Never).\n  - Verify your command/entrypoint doesn\u2019t exit immediately.\n\n2) Pending (Unschedulable)\n- Causes: Not enough CPU/memory, nodeSelector/affinity/taints don\u2019t match any node, PVC is Pending.\n- Check: kubectl describe pod \u2026 (look for \u201c0/1 nodes are available\u201d reasons), kubectl get pvc -n <ns>.\n- Fix: Adjust requests/limits, taints/tolerations, selectors/affinity, or provision storage.\n\n3) ContainerCreating or Init:0/1\n- Causes: CNI/network not ready, image pull secrets, missing ConfigMap/Secret/Volume, init container failing.\n- Check: kubectl describe pod \u2026 (events at bottom), kubectl logs <pod> -c <init-container>.\n- Fix: Resolve image/auth, ensure referenced volumes and secrets/configmaps exist, check CNI/Node readiness.\n\n4) ImagePullBackOff / ErrImagePull\n- Causes: Wrong image name/tag or registry auth.\n- Fix: Correct the image or imagePullSecrets; test docker/podman pull manually.\n\n5) CrashLoopBackOff\n- Causes: Your app is crashing or failing health checks.\n- Check: kubectl logs <pod> --previous, and current logs; verify liveness/readiness probes.\n- Fix: Fix app config, delay start, adjust probes, ensure dependencies available.\n\n6) Running but NotReady\n- Causes: Readiness probe failing, dependency not reachable, port not listening.\n- Check: kubectl describe pod \u2026 (probe failures).\n- Fix: Fix probe path/port, ensure the service/dependency is reachable.\n\n7) Terminating and won\u2019t go away\n- Causes: Finalizers, stuck volume detach, CNI teardown, node unreachable.\n- Check: kubectl describe pod \u2026 (finalizers/events), kubectl get node.\n- Fix: Remove finalizers carefully if appropriate, or force delete: kubectl delete pod <pod> --grace-period=0 --force -n <ns> (last resort).\n\nHeads-ups related to your context:\n- If you edited /etc/hosts inside the container, kubelet will overwrite it. Use spec.hostAliases or DNS instead. Manual changes are not persistent.\n- Dual-stack Services: If your Service defaulted to a single IPv6 ClusterIP and your client/probes are IPv4-only, readiness may fail and look \u201cstuck.\u201d Specify ipFamilyPolicy: PreferDualStack and ipFamilies: [IPv4, IPv6] if you need both, or force IPv4.\n\nIf you paste the describe and logs output, I can pinpoint the exact cause and fix."
    }
  }
}