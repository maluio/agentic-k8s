{
  "Why is my pod stuck in CrashLoopBackOff?": {
    "gpt-5-nano_base": {
      "input": {
        "question": "Why is my pod stuck in CrashLoopBackOff?",
        "prompt": "You are a Kubernetes expert. Be concise and accurate.\n\nWhy is my pod stuck in CrashLoopBackOff?",
        "system_prompt": "You are a Kubernetes expert. Be concise and accurate.",
        "human_prompt": "Why is my pod stuck in CrashLoopBackOff?"
      },
      "response": "CrashLoopBackOff means the container in your Pod starts, then exits, and the kubelet keeps restarting it with exponential backoff. It\u2019s almost always your process crashing or being killed (including by probes or OOM), not an image pull issue.\n\nQuick triage\n- Get status and events: kubectl describe pod <pod>\n- See the last termination reason/exit code: kubectl get pod <pod> -o jsonpath='{range .status.containerStatuses[*]}{.name} {.lastState.terminated.reason} {.lastState.terminated.exitCode}{\"\\n\"}{end}'\n- Check logs of the current and previous container: kubectl logs <pod> -c <container>; kubectl logs -p <pod> -c <container>\n- If there are initContainers: kubectl logs <pod> -c <init-name>\n\nCommon causes and what to look for\n- Bad command/entrypoint or app exits immediately\n  - Symptoms: exit code 1, \u201cexec: not found\u201d, \u201cpermission denied\u201d, or the app just prints help and exits.\n  - Fix: correct command/args, make the binary executable, keep the main process in foreground for Deployments.\n- Liveness/startup/readiness probes misconfigured\n  - Symptoms: describe shows \u201cLiveness probe failed\u201d; restarts line up with probe period.\n  - Fix: verify port/path/scheme, increase initialDelaySeconds/timeoutSeconds, add a startupProbe for slow boots.\n- OOMKilled (out of memory)\n  - Symptoms: exit code 137, reason OOMKilled in describe/lastState.\n  - Fix: increase memory limit, tune app memory (e.g., JVM -Xmx), reduce workload, check memory leaks.\n- Missing config/secrets/env or bad mounts\n  - Symptoms: app logs show \u201cmissing $VAR/secret/config\u201d; crash at startup.\n  - Fix: ensure ConfigMap/Secret keys exist and are mounted/exposed correctly; correct mount paths and filenames.\n- File system/permission/security context issues\n  - Symptoms: \u201cpermission denied\u201d, can\u2019t bind to port <1024>, can\u2019t write to root FS or mounted volume.\n  - Fix: run as appropriate user or add capabilities (e.g., CAP_NET_BIND_SERVICE), use a writable emptyDir, set fsGroup/chown via initContainer, disable readOnlyRootFilesystem if needed.\n- Crashing bug in the app\n  - Symptoms: exit code 139 (segfault) or stack traces.\n  - Fix: fix the app, upgrade image, or capture a core dump.\n\nTips to break the loop and debug\n- Temporarily disable livenessProbe or add a generous startupProbe.\n- Start a debug shell without changing the image: kubectl debug -it <pod> --image=busybox --target=<container>\n- Override the command to keep the container alive for inspection (e.g., sleep) via a temporary patch or a separate debug Deployment.\n- Deleting the Pod only resets the backoff; it won\u2019t fix the root cause.\n\nIf you share the output of:\n- kubectl describe pod <pod>\n- kubectl logs -p <pod> -c <container>\nI can pinpoint the exact cause and the fix."
    },
    "gpt-5-nano_rag": {
      "input": {
        "question": "Why is my pod stuck in CrashLoopBackOff?",
        "context": "In practice, a CrashLoopBackOff is a condition or event that might be seen as output from the kubectl command, while describing or listing Pods, when a container in the Pod fails to start properly and then continually tries and fails in a loop.\n\nIn other words, when a container enters the crash loop, Kubernetes applies the exponential backoff delay mentioned in the Container restart policy. This mechanism prevents a faulty container from overwhelming the system with continuous failed start attempts.\n\nThe CrashLoopBackOff can be caused by issues like the following:\n\nApplication errors that cause the container to exit.\n\nConfiguration errors, such as incorrect environment variables or missing configuration files.\n\nResource constraints, where the container might not have enough memory or CPU to start properly.\n\nHealth checks failing if the application doesn't start serving within the expected time.\n\nHealth checks failing if the application doesn't start serving within the expected time.\n\nContainer liveness probes or startup probes returning a Failure result as mentioned in the probes section.\n\nTo investigate the root cause of a CrashLoopBackOff issue, a user can:\n\nCheck logs: Use kubectl logs <name-of-pod> to check the logs of the container. This is often the most direct way to diagnose the issue causing the crashes.\n\nInspect events: Use kubectl describe pod <name-of-pod> to see events for the Pod, which can provide hints about configuration or resource issues.\n\nReview configuration: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct and that all required external resources are available.\n\nCheck resource limits: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the resources in the Pod definition can resolve the issue.\n\nHow Pods handle problems with containers {#container-restarts}\n\nKubernetes manages container failures within Pods using a restartPolicy defined in the Pod spec. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:\n\nInitial crash: Kubernetes attempts an immediate restart based on the Pod restartPolicy.\n\nRepeated crashes: After the initial crash Kubernetes applies an exponential backoff delay for subsequent restarts, described in restartPolicy. This prevents rapid, repeated restart attempts from overloading the system.\n\nCrashLoopBackOff state: This indicates that the backoff delay mechanism is currently in effect for a given container that is in a crash loop, failing and restarting repeatedly.\n\nBackoff reset: If a container runs successfully for a certain duration (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash as the first one.\n\n{{< note >}}\n\nWhen a pod is failing to start repeatedly, CrashLoopBackOff may appear in the Status field of some kubectl commands. Similarly, when a pod is being deleted, Terminating may appear in the Status field of some kubectl commands.\n\nMake sure not to confuse Status, a kubectl display field for user intuition, with the pod's phase. Pod phase is an explicit part of the Kubernetes data model and of the Pod API.\n\n  NAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\n  alessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n\nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds. You can use the flag --force to terminate a Pod by force. {{< /note >}}",
        "context_docs": [
          {
            "content": "In practice, a CrashLoopBackOff is a condition or event that might be seen as output from the kubectl command, while describing or listing Pods, when a container in the Pod fails to start properly and then continually tries and fails in a loop.\n\nIn other words, when a container enters the crash loop, Kubernetes applies the exponential backoff delay mentioned in the Container restart policy. This mechanism prevents a faulty container from overwhelming the system with continuous failed start attempts.\n\nThe CrashLoopBackOff can be caused by issues like the following:\n\nApplication errors that cause the container to exit.\n\nConfiguration errors, such as incorrect environment variables or missing configuration files.\n\nResource constraints, where the container might not have enough memory or CPU to start properly.\n\nHealth checks failing if the application doesn't start serving within the expected time.",
            "metadata": {
              "source": "/Users/dev/Documents/proj/dev/agentic-k8s/website/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
              "_id": "1cb3406f-0d50-43e4-b7fb-a6ed7f9a839c",
              "_collection_name": "k8s_docs"
            }
          },
          {
            "content": "Health checks failing if the application doesn't start serving within the expected time.\n\nContainer liveness probes or startup probes returning a Failure result as mentioned in the probes section.\n\nTo investigate the root cause of a CrashLoopBackOff issue, a user can:\n\nCheck logs: Use kubectl logs <name-of-pod> to check the logs of the container. This is often the most direct way to diagnose the issue causing the crashes.\n\nInspect events: Use kubectl describe pod <name-of-pod> to see events for the Pod, which can provide hints about configuration or resource issues.\n\nReview configuration: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct and that all required external resources are available.\n\nCheck resource limits: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the resources in the Pod definition can resolve the issue.",
            "metadata": {
              "source": "/Users/dev/Documents/proj/dev/agentic-k8s/website/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
              "_id": "078b4230-29e3-4190-88e4-6da3cabb2521",
              "_collection_name": "k8s_docs"
            }
          },
          {
            "content": "How Pods handle problems with containers {#container-restarts}\n\nKubernetes manages container failures within Pods using a restartPolicy defined in the Pod spec. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:\n\nInitial crash: Kubernetes attempts an immediate restart based on the Pod restartPolicy.\n\nRepeated crashes: After the initial crash Kubernetes applies an exponential backoff delay for subsequent restarts, described in restartPolicy. This prevents rapid, repeated restart attempts from overloading the system.\n\nCrashLoopBackOff state: This indicates that the backoff delay mechanism is currently in effect for a given container that is in a crash loop, failing and restarting repeatedly.\n\nBackoff reset: If a container runs successfully for a certain duration (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash as the first one.",
            "metadata": {
              "source": "/Users/dev/Documents/proj/dev/agentic-k8s/website/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
              "_id": "ba14b4c0-b01e-4be9-b28e-edf1262813a4",
              "_collection_name": "k8s_docs"
            }
          },
          {
            "content": "{{< note >}}\n\nWhen a pod is failing to start repeatedly, CrashLoopBackOff may appear in the Status field of some kubectl commands. Similarly, when a pod is being deleted, Terminating may appear in the Status field of some kubectl commands.\n\nMake sure not to confuse Status, a kubectl display field for user intuition, with the pod's phase. Pod phase is an explicit part of the Kubernetes data model and of the Pod API.\n\n  NAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\n  alessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n\nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds. You can use the flag --force to terminate a Pod by force. {{< /note >}}",
            "metadata": {
              "source": "/Users/dev/Documents/proj/dev/agentic-k8s/website/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
              "_id": "4205bdce-5fa2-41f8-b247-6319fb7a9021",
              "_collection_name": "k8s_docs"
            }
          }
        ],
        "prompt": "You are a Kubernetes expert. Use this context:\nIn practice, a CrashLoopBackOff is a condition or event that might be seen as output from the kubectl command, while describing or listing Pods, when a container in the Pod fails to start properly and then continually tries and fails in a loop.\n\nIn other words, when a container enters the crash loop, Kubernetes applies the exponential backoff delay mentioned in the Container restart policy. This mechanism prevents a faulty container from overwhelming the system with continuous failed start attempts.\n\nThe CrashLoopBackOff can be caused by issues like the following:\n\nApplication errors that cause the container to exit.\n\nConfiguration errors, such as incorrect environment variables or missing configuration files.\n\nResource constraints, where the container might not have enough memory or CPU to start properly.\n\nHealth checks failing if the application doesn't start serving within the expected time.\n\nHealth checks failing if the application doesn't start serving within the expected time.\n\nContainer liveness probes or startup probes returning a Failure result as mentioned in the probes section.\n\nTo investigate the root cause of a CrashLoopBackOff issue, a user can:\n\nCheck logs: Use kubectl logs <name-of-pod> to check the logs of the container. This is often the most direct way to diagnose the issue causing the crashes.\n\nInspect events: Use kubectl describe pod <name-of-pod> to see events for the Pod, which can provide hints about configuration or resource issues.\n\nReview configuration: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct and that all required external resources are available.\n\nCheck resource limits: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the resources in the Pod definition can resolve the issue.\n\nHow Pods handle problems with containers {#container-restarts}\n\nKubernetes manages container failures within Pods using a restartPolicy defined in the Pod spec. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:\n\nInitial crash: Kubernetes attempts an immediate restart based on the Pod restartPolicy.\n\nRepeated crashes: After the initial crash Kubernetes applies an exponential backoff delay for subsequent restarts, described in restartPolicy. This prevents rapid, repeated restart attempts from overloading the system.\n\nCrashLoopBackOff state: This indicates that the backoff delay mechanism is currently in effect for a given container that is in a crash loop, failing and restarting repeatedly.\n\nBackoff reset: If a container runs successfully for a certain duration (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash as the first one.\n\n{{< note >}}\n\nWhen a pod is failing to start repeatedly, CrashLoopBackOff may appear in the Status field of some kubectl commands. Similarly, when a pod is being deleted, Terminating may appear in the Status field of some kubectl commands.\n\nMake sure not to confuse Status, a kubectl display field for user intuition, with the pod's phase. Pod phase is an explicit part of the Kubernetes data model and of the Pod API.\n\n  NAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\n  alessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n\nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds. You can use the flag --force to terminate a Pod by force. {{< /note >}}\n\nWhy is my pod stuck in CrashLoopBackOff?",
        "system_prompt": "You are a Kubernetes expert. Use this context:\nIn practice, a CrashLoopBackOff is a condition or event that might be seen as output from the kubectl command, while describing or listing Pods, when a container in the Pod fails to start properly and then continually tries and fails in a loop.\n\nIn other words, when a container enters the crash loop, Kubernetes applies the exponential backoff delay mentioned in the Container restart policy. This mechanism prevents a faulty container from overwhelming the system with continuous failed start attempts.\n\nThe CrashLoopBackOff can be caused by issues like the following:\n\nApplication errors that cause the container to exit.\n\nConfiguration errors, such as incorrect environment variables or missing configuration files.\n\nResource constraints, where the container might not have enough memory or CPU to start properly.\n\nHealth checks failing if the application doesn't start serving within the expected time.\n\nHealth checks failing if the application doesn't start serving within the expected time.\n\nContainer liveness probes or startup probes returning a Failure result as mentioned in the probes section.\n\nTo investigate the root cause of a CrashLoopBackOff issue, a user can:\n\nCheck logs: Use kubectl logs <name-of-pod> to check the logs of the container. This is often the most direct way to diagnose the issue causing the crashes.\n\nInspect events: Use kubectl describe pod <name-of-pod> to see events for the Pod, which can provide hints about configuration or resource issues.\n\nReview configuration: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct and that all required external resources are available.\n\nCheck resource limits: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the resources in the Pod definition can resolve the issue.\n\nHow Pods handle problems with containers {#container-restarts}\n\nKubernetes manages container failures within Pods using a restartPolicy defined in the Pod spec. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:\n\nInitial crash: Kubernetes attempts an immediate restart based on the Pod restartPolicy.\n\nRepeated crashes: After the initial crash Kubernetes applies an exponential backoff delay for subsequent restarts, described in restartPolicy. This prevents rapid, repeated restart attempts from overloading the system.\n\nCrashLoopBackOff state: This indicates that the backoff delay mechanism is currently in effect for a given container that is in a crash loop, failing and restarting repeatedly.\n\nBackoff reset: If a container runs successfully for a certain duration (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash as the first one.\n\n{{< note >}}\n\nWhen a pod is failing to start repeatedly, CrashLoopBackOff may appear in the Status field of some kubectl commands. Similarly, when a pod is being deleted, Terminating may appear in the Status field of some kubectl commands.\n\nMake sure not to confuse Status, a kubectl display field for user intuition, with the pod's phase. Pod phase is an explicit part of the Kubernetes data model and of the Pod API.\n\n  NAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\n  alessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n\nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds. You can use the flag --force to terminate a Pod by force. {{< /note >}}",
        "human_prompt": "Why is my pod stuck in CrashLoopBackOff?"
      },
      "response": "CrashLoopBackOff means the container in your Pod is repeatedly starting, crashing, and Kubernetes is backing off before trying again. It\u2019s a symptom, not the cause. The usual root causes are:\n\n- The app exits on startup (bad command/args, missing files/env, bug/exception)\n- Liveness or startup probes failing and forcing restarts\n- Out-of-memory kills (memory limit too low or leak); less often CPU starvation causing probe timeouts\n- Permissions or runtime issues (non-executable entrypoint, wrong user, cannot bind to privileged port, read-only FS)\n- Misconfigured mounts, ConfigMaps, or Secrets; missing external dependencies causing the app to exit\n\nQuick triage (fastest signal first)\n1) Describe the pod for reasons, exit codes, and events:\n- kubectl describe pod <pod> -n <ns>\n- Look under Containers -> State/Last State:\n  - Reason=OOMKilled \u2192 memory issue\n  - Reason=Error with Exit Code (e.g., 1, 126, 127, 139) \u2192 app/command failure\n  - Probe failures visible in Events: \u201cLiveness probe failed \u2026\u201d\n2) Get logs from the last crashed instance:\n- kubectl logs <pod> -n <ns> -c <container> --previous\n- If multiple containers, check each -c\n3) Check your config and resources:\n- kubectl get pod <pod> -n <ns> -o yaml (review command/args, env, mounts, probes, resources)\n- Ensure referenced ConfigMaps/Secrets keys match and volumes mount where the app expects\n- Verify resource limits (especially memory)\n\nHow to interpret and fix common findings\n- OOMKilled:\n  - Increase memory limit/requests; tune app memory (e.g., JVM -Xmx)\n  - Investigate leaks; reduce workload at startup\n- Liveness/startup probes failing:\n  - Verify the endpoint/command is correct and reachable\n  - Use startupProbe to give the app warm-up time\n  - Increase initialDelaySeconds/timeoutSeconds/failureThreshold\n- Command/entrypoint errors (Exit 127/126/Permission denied):\n  - Fix image ENTRYPOINT/CMD and args\n  - Ensure scripts are executable and have correct shebang; avoid CRLF line endings\n  - Use absolute paths; verify workingDir\n- App exits due to config/dependency:\n  - Validate env vars and file paths\n  - Ensure required files from ConfigMap/Secret exist at the expected path\n  - If external services aren\u2019t ready, make the app retry instead of exiting; or use initContainers\n- Permission/user issues:\n  - If binding <1024, run as root or add CAP_NET_BIND_SERVICE, or use a higher port\n  - Check securityContext (runAsUser, fsGroup, readOnlyRootFilesystem) vs file ownership\n- Resource pressure/timeouts:\n  - If CPU limits are tight, probes may time out; raise limits or relax probe timeouts\n\nHelpful commands/snippets\n- Events (sorted):\n  - kubectl get events -n <ns> --sort-by=.lastTimestamp\n- Container statuses quickly:\n  - kubectl get pod <pod> -n <ns> -o jsonpath='{.status.containerStatuses[*]}'\n- Get logs continuously when it does start briefly:\n  - kubectl logs -f <pod> -n <ns> -c <container>\n- If it dies too fast to inspect, add a temporary sleep:\n  - Patch Deployment/Pod to command: [\"sh\",\"-c\",\"sleep 3600 && exec <original>\"] to get a shell in\n  - Or use kubectl debug --target <container> -n <ns> <pod> to run an ephemeral debug container\n\nNotes\n- CrashLoopBackOff is a status reason shown by kubectl; the actual root cause is in the container\u2019s Last State, exit code, logs, and events.\n- The backoff resets after the container runs successfully for a while (~10 minutes).\n\nIf you share the output of:\n- kubectl describe pod <pod> -n <ns>\n- kubectl logs <pod> -n <ns> -c <container> --previous\nI can help pinpoint the exact cause and the minimal fix."
    }
  }
}