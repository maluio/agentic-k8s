{
  "metadata": {
    "timestamp": "2025-12-12T08:11:37.905459",
    "parameters": {
      "docs_path": "k8s-docs/content/en/docs/tasks/network",
      "tag": null,
      "skip_index": false,
      "chunk_size": 1000,
      "chunk_overlap": 100,
      "recreate_index": true,
      "collection_name": "k8s_docs",
      "qdrant_url": "http://localhost:6333",
      "models": "openai/gpt-5-nano",
      "retriever_k": 4,
      "mode": "both",
      "show_context": false,
      "output": "./evaluations/evaluation-2025-12-12-08-11.json",
      "questions_count": 1
    }
  },
  "results": {
    "Why is my pod stuck in CrashLoopBackOff?": {
      "openai/gpt-5-nano_base": {
        "input": {
          "question": "Why is my pod stuck in CrashLoopBackOff?",
          "prompt": "You are a Kubernetes expert. Be concise and accurate.\n\nWhy is my pod stuck in CrashLoopBackOff?",
          "system_prompt": "You are a Kubernetes expert. Be concise and accurate.",
          "human_prompt": "Why is my pod stuck in CrashLoopBackOff?"
        },
        "response": "CrashLoopBackOff means a container in your pod is starting, then exiting, and Kubernetes keeps restarting it. The cause is almost always one of the following: the app itself crashes, a startup/config issue, a probe misconfiguration, or resource/volume problems.\n\nKeen triage steps (run these in order)\n\n1) Get the pod and its status\n- kubectl get pod <pod> -n <ns>\n- kubectl describe pod <pod> -n <ns>  # look for Events, RestartCount, and container state\n\n2) Check why the container terminated\n- kubectl get pod <pod> -n <ns> -o jsonpath='{.status.containerStatuses[*].name}'\n- kubectl get pod <pod> -n <ns> -o jsonpath='{.status.containerStatuses[*].lastTerminationState.terminated}'\n- kubectl get pod <pod> -n <ns> -o jsonpath='{.status.containerStatuses[*].lastTerminationState.terminated.ExitCode}'\n- Look for ExitCode (non-zero) and Reason (e.g., Error, OOMKilled, ExitCode)\n\n3) Inspect logs\n- kubectl logs <pod> -n <ns>  # latest container logs\n- If there are multiple containers, specify the name: kubectl logs <pod> -n <ns> -c <container>\n- To see what happened on the previous crash: kubectl logs <pod> -n <ns> -c <container> --previous\n\n4) Check for common culprits\n- Application crash or misconfiguration: non-zero exit code, stack trace in logs.\n- Missing/invalid config or secrets: env vars, configMap, or secret mounts not present or misnamed.\n- Probes causing restarts: livenessProbe or startupProbe failing, or a too-aggressive initialDelaySeconds.\n- Resource limits: OOMKilled (look for lastTerminationState.Reason: OOMKilled) or CPU/Mem pressure.\n- Init container failures or volume mounts: ensure volumes/paths exist and permissions are correct.\n- Image or command issues: entrypoint/command script failing, or image not pulled correctly.\n\n5) Check probes and config\n- kubectl describe pod <pod> -n <ns> and inspect:\n  - Liveness and startup probes: are their thresholds too strict or initial delays too short?\n  - Mounted volumes: are Secrets/ConfigMaps present and mounted at the expected paths?\n  - Environment: are required env vars provided?\n\n6) If you can\u2019t find a clear cause\n- Compare to a working pod/deployment: what changed recently (image, config, probes, resources)?\n- Temporarily relax probes to see if restarts stop (e.g., disable livenessProbe or set higher initialDelaySeconds/timeout). Then revert after you identify the cause.\n- Increase resources if you suspect OOM or CPU issues (kubectl edit deployment to adjust resources).\n\nHelpful one-liners you can paste back here with results\n- Pod state and restart counts:\n  kubectl get pod <pod> -n <ns> -o wide\n- Detailed events:\n  kubectl describe pod <pod> -n <ns>\n- Exit code and termination state:\n  kubectl get pod <pod> -n <ns> -o jsonpath='{.status.containerStatuses[*].lastTerminationState.terminated.ExitCode}{\"\\t\"}{.status.containerStatuses[*].lastTerminationState.terminated.Reason}{\"\\n\"}'\n- Previous logs:\n  kubectl logs <pod> -n <ns> -c <container> --previous\n\nIf you want, share:\n- pod name, namespace\n- the container name (if multiple)\n- the output of kubectl describe pod <pod> -n <ns>\n- the relevant logs (latest and --previous)\n\nI can help pinpoint the exact cause and suggest concrete fixes."
      },
      "openai/gpt-5-nano_rag": {
        "input": {
          "question": "Why is my pod stuck in CrashLoopBackOff?",
          "context": "pod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nHostname: k8s-linuxpool1-34450317-0\nInternalIP: 10.0.0.5\nInternalIP: 2001:db8:10::5\n\nValidate Pod addressing\n\nValidate that a Pod has an IPv4 and IPv6 address assigned. Replace the Pod name with a valid Pod in your cluster. In this example the Pod name is pod01:\n\nkubectl get pods pod01 -o go-template --template='{{range .status.podIPs}}{{printf \"%s\\n\" .ip}}{{end}}'\n\n10.244.1.4\n2001:db8::4\n\nYou can also validate Pod IPs using the Downward API via the status.podIPs fieldPath. The following snippet demonstrates how you can expose the Pod IPs via an environment variable called MY_POD_IPS within a container.\n\n        env:\n        - name: MY_POD_IPS\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIPs\n\nThe following command prints the value of the MY_POD_IPS environment variable from within a container. The value is a comma separated list that corresponds to the Pod's IPv4 and IPv6 addresses.\n\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.\n\nValidate addressing\n\nValidate node addressing\n\nEach dual-stack Node should have a single IPv4 block and a single IPv6 block allocated. Validate that IPv4/IPv6 Pod address ranges are configured by running the following command. Replace the sample node name with a valid dual-stack Node from your cluster. In this example, the Node's name is k8s-linuxpool1-34450317-0:\n\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template='{{range .spec.podCIDRs}}{{printf \"%s\\n\" .}}{{end}}'\n\n10.244.1.0/24\n2001:db8::/64\n\nThere should be one IPv4 block and one IPv6 block allocated.\n\nValidate that the node has an IPv4 and IPv6 interface detected. Replace node name with a valid node from the cluster. In this example the node name is k8s-linuxpool1-34450317-0:\n\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template='{{range .status.addresses}}{{printf \"%s: %s\\n\" .type .address}}{{end}}'\n\nHostname: k8s-linuxpool1-34450317-0\nInternalIP: 10.0.0.5\nInternalIP: 2001:db8:10::5",
          "context_docs": [
            {
              "content": "pod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}",
              "metadata": {
                "source": "k8s-docs/content/en/docs/tasks/network/customize-hosts-file-for-pods.md",
                "_id": "f1eb523f-1931-4f88-809e-4ee823c8cb85",
                "_collection_name": "k8s_docs"
              }
            },
            {
              "content": "Hostname: k8s-linuxpool1-34450317-0\nInternalIP: 10.0.0.5\nInternalIP: 2001:db8:10::5\n\nValidate Pod addressing\n\nValidate that a Pod has an IPv4 and IPv6 address assigned. Replace the Pod name with a valid Pod in your cluster. In this example the Pod name is pod01:\n\nkubectl get pods pod01 -o go-template --template='{{range .status.podIPs}}{{printf \"%s\\n\" .ip}}{{end}}'\n\n10.244.1.4\n2001:db8::4\n\nYou can also validate Pod IPs using the Downward API via the status.podIPs fieldPath. The following snippet demonstrates how you can expose the Pod IPs via an environment variable called MY_POD_IPS within a container.\n\n        env:\n        - name: MY_POD_IPS\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIPs\n\nThe following command prints the value of the MY_POD_IPS environment variable from within a container. The value is a comma separated list that corresponds to the Pod's IPv4 and IPv6 addresses.\n\nkubectl exec -it pod01 -- set | grep MY_POD_IPS",
              "metadata": {
                "source": "k8s-docs/content/en/docs/tasks/network/validate-dual-stack.md",
                "_id": "82a77eb2-22cb-4580-a920-cb456ec328f1",
                "_collection_name": "k8s_docs"
              }
            },
            {
              "content": "kubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.",
              "metadata": {
                "source": "k8s-docs/content/en/docs/tasks/network/validate-dual-stack.md",
                "_id": "ae6b69e2-0dab-4ac7-ae55-ca35ffe3ce25",
                "_collection_name": "k8s_docs"
              }
            },
            {
              "content": "Validate addressing\n\nValidate node addressing\n\nEach dual-stack Node should have a single IPv4 block and a single IPv6 block allocated. Validate that IPv4/IPv6 Pod address ranges are configured by running the following command. Replace the sample node name with a valid dual-stack Node from your cluster. In this example, the Node's name is k8s-linuxpool1-34450317-0:\n\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template='{{range .spec.podCIDRs}}{{printf \"%s\\n\" .}}{{end}}'\n\n10.244.1.0/24\n2001:db8::/64\n\nThere should be one IPv4 block and one IPv6 block allocated.\n\nValidate that the node has an IPv4 and IPv6 interface detected. Replace node name with a valid node from the cluster. In this example the node name is k8s-linuxpool1-34450317-0:\n\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template='{{range .status.addresses}}{{printf \"%s: %s\\n\" .type .address}}{{end}}'\n\nHostname: k8s-linuxpool1-34450317-0\nInternalIP: 10.0.0.5\nInternalIP: 2001:db8:10::5",
              "metadata": {
                "source": "k8s-docs/content/en/docs/tasks/network/validate-dual-stack.md",
                "_id": "d5d620d0-c505-4acf-8fb1-87f4f181dd95",
                "_collection_name": "k8s_docs"
              }
            }
          ],
          "prompt": "You are a Kubernetes expert. Use this context:\npod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nHostname: k8s-linuxpool1-34450317-0\nInternalIP: 10.0.0.5\nInternalIP: 2001:db8:10::5\n\nValidate Pod addressing\n\nValidate that a Pod has an IPv4 and IPv6 address assigned. Replace the Pod name with a valid Pod in your cluster. In this example the Pod name is pod01:\n\nkubectl get pods pod01 -o go-template --template='{{range .status.podIPs}}{{printf \"%s\\n\" .ip}}{{end}}'\n\n10.244.1.4\n2001:db8::4\n\nYou can also validate Pod IPs using the Downward API via the status.podIPs fieldPath. The following snippet demonstrates how you can expose the Pod IPs via an environment variable called MY_POD_IPS within a container.\n\n        env:\n        - name: MY_POD_IPS\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIPs\n\nThe following command prints the value of the MY_POD_IPS environment variable from within a container. The value is a comma separated list that corresponds to the Pod's IPv4 and IPv6 addresses.\n\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.\n\nValidate addressing\n\nValidate node addressing\n\nEach dual-stack Node should have a single IPv4 block and a single IPv6 block allocated. Validate that IPv4/IPv6 Pod address ranges are configured by running the following command. Replace the sample node name with a valid dual-stack Node from your cluster. In this example, the Node's name is k8s-linuxpool1-34450317-0:\n\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template='{{range .spec.podCIDRs}}{{printf \"%s\\n\" .}}{{end}}'\n\n10.244.1.0/24\n2001:db8::/64\n\nThere should be one IPv4 block and one IPv6 block allocated.\n\nValidate that the node has an IPv4 and IPv6 interface detected. Replace node name with a valid node from the cluster. In this example the node name is k8s-linuxpool1-34450317-0:\n\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template='{{range .status.addresses}}{{printf \"%s: %s\\n\" .type .address}}{{end}}'\n\nHostname: k8s-linuxpool1-34450317-0\nInternalIP: 10.0.0.5\nInternalIP: 2001:db8:10::5\n\nWhy is my pod stuck in CrashLoopBackOff?",
          "system_prompt": "You are a Kubernetes expert. Use this context:\npod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nHostname: k8s-linuxpool1-34450317-0\nInternalIP: 10.0.0.5\nInternalIP: 2001:db8:10::5\n\nValidate Pod addressing\n\nValidate that a Pod has an IPv4 and IPv6 address assigned. Replace the Pod name with a valid Pod in your cluster. In this example the Pod name is pod01:\n\nkubectl get pods pod01 -o go-template --template='{{range .status.podIPs}}{{printf \"%s\\n\" .ip}}{{end}}'\n\n10.244.1.4\n2001:db8::4\n\nYou can also validate Pod IPs using the Downward API via the status.podIPs fieldPath. The following snippet demonstrates how you can expose the Pod IPs via an environment variable called MY_POD_IPS within a container.\n\n        env:\n        - name: MY_POD_IPS\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIPs\n\nThe following command prints the value of the MY_POD_IPS environment variable from within a container. The value is a comma separated list that corresponds to the Pod's IPv4 and IPv6 addresses.\n\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.\n\nValidate addressing\n\nValidate node addressing\n\nEach dual-stack Node should have a single IPv4 block and a single IPv6 block allocated. Validate that IPv4/IPv6 Pod address ranges are configured by running the following command. Replace the sample node name with a valid dual-stack Node from your cluster. In this example, the Node's name is k8s-linuxpool1-34450317-0:\n\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template='{{range .spec.podCIDRs}}{{printf \"%s\\n\" .}}{{end}}'\n\n10.244.1.0/24\n2001:db8::/64\n\nThere should be one IPv4 block and one IPv6 block allocated.\n\nValidate that the node has an IPv4 and IPv6 interface detected. Replace node name with a valid node from the cluster. In this example the node name is k8s-linuxpool1-34450317-0:\n\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template='{{range .status.addresses}}{{printf \"%s: %s\\n\" .type .address}}{{end}}'\n\nHostname: k8s-linuxpool1-34450317-0\nInternalIP: 10.0.0.5\nInternalIP: 2001:db8:10::5",
          "human_prompt": "Why is my pod stuck in CrashLoopBackOff?"
        },
        "response": "A CrashLoopBackOff means a container keeps crashing and Kubernetes keeps restarting it. Based on the details you showed, your pod isn\u2019t in CrashLoopBackOff \u2014 it\u2019s showing Completed:\n\n- Pod hostaliases-pod: READY 0/1, STATUS Completed, IP 10.200.0.5\n- This indicates the container ran and finished (likely a short task), not a failing, constantly restarting container.\n\nIf you are actually seeing CrashLoopBackOff for some pod, here\u2019s how to troubleshoot quickly:\n\nWhat to check\n- kubectl describe pod <pod-name>:\n  - Look for restartCount, reason, and events. Back-off messages, OOMKilled, ImagePullBackOff, or CrashLoopBackOff will appear here.\n- kubectl logs <pod-name> [ -c <container> ]:\n  - Check the current run logs for the crash cause.\n- If the pod has previously crashed, view the previous logs:\n  - kubectl logs <pod-name> -p\n- Check the container\u2019s exit state:\n  - kubectl get pod <pod-name> -o jsonpath='{.status.containerStatuses[0].state.terminated.exitCode}'\n  - If there was a crash, you\u2019ll see a non-zero exit code and possibly .lastTerminationState.terminated.message with a hint.\n\nCommon causes and fixes\n- The container exits immediately (the command/entrypoint finishes):\n  - If you want a long-running pod, ensure the container runs a foreground process (e.g., a server loop) and does not exit.\n  - If this is intended (e.g., a Job), leave it as is or create a Job/CronJob instead of a Deployment/Pod.\n- Application crash due to code error or bad config:\n  - Check your command/args, environment variables, ConfigMaps/Secrets, and mounted volumes.\n- Probes failing (liveness/readiness):\n  - Misconfigured probes can cause restarts; verify probe paths, ports, and timings.\n- Resource issues (OOM, CPU):\n  - Check limits/requests and node resources; an OOMKilled restart shows in events.\n- Image or command issues:\n  - Verify the image name, tag, and that the image actually starts a long-running process.\n  - If you override CMD/ENTRYPOINT, ensure it\u2019s appropriate for a long-running container.\n\nShort example triage commands\n- Get a quick status and events:\n  - kubectl describe pod hostaliases-pod\n- See current and previous logs:\n  - kubectl logs hostaliases-pod\n  - kubectl logs hostaliases-pod -p\n- Check if it\u2019s actually a Completed/Succeeded pod vs CrashLoopBackOff:\n  - kubectl get pod hostaliases-pod -o wide\n  - kubectl get pod hostaliases-pod -o jsonpath='{.status.phase}'\n\nIf you want, paste the pod name, the output of kubectl describe pod <pod>, and the logs, and I\u2019ll help pinpoint the exact cause."
      }
    }
  }
}