{
  "metadata": {
    "timestamp": "2025-12-11T13:01:31.426649",
    "parameters": {
      "docs_path": "k8s-docs/content/en/docs/tasks/network",
      "tag": "snapshot-initial-v1.30",
      "skip_index": false,
      "chunk_size": 1000,
      "chunk_overlap": 100,
      "recreate_index": true,
      "collection_name": "k8s_docs",
      "qdrant_url": "http://localhost:6333",
      "models": "gpt-5-nano",
      "retriever_k": 4,
      "mode": "both",
      "show_context": false,
      "output": "./evaluations/evaluation-2025-12-11-13-01.json",
      "questions_count": 1
    }
  },
  "results": {
    "Why is my pod stuck?": {
      "gpt-5-nano_base": {
        "input": {
          "question": "Why is my pod stuck?",
          "prompt": "You are a Kubernetes expert. Be concise and accurate.\n\nWhy is my pod stuck?",
          "system_prompt": "You are a Kubernetes expert. Be concise and accurate.",
          "human_prompt": "Why is my pod stuck?"
        },
        "response": "\u201cStuck\u201d usually means the pod isn\u2019t progressing past a phase. Start by finding the exact phase and reason.\n\n1) Get the facts\n- kubectl get pod <name> -n <ns> -o wide\n- kubectl describe pod <name> -n <ns>\n- kubectl get events -n <ns> --sort-by=.lastTimestamp | tail -n 50\n- For logs:\n  - init containers: kubectl logs <pod> -n <ns> -c <init-container>\n  - app container (last crash): kubectl logs <pod> -n <ns> -c <container> --previous\n\n2) Common states, causes, and fixes\n\nPending (not scheduled)\n- Reasons in describe: \u201c0/N nodes are available: \u2026\u201d\n  - insufficient cpu/memory or ephemeral-storage \u2192 lower requests or add node capacity.\n  - node affinity/selector/topology prevents scheduling \u2192 adjust nodeSelector/affinity/topologySpread.\n  - node taints present \u2192 add tolerations or choose taint-free nodes.\n  - unbound PVCs \u2192 kubectl get pvc -n <ns>; ensure StorageClass exists, size/accessModes match, zones match (WaitForFirstConsumer may delay binding).\n  - ResourceQuota/LimitRange blocking \u2192 kubectl describe quota, limitrange -n <ns> and adjust requests/limits.\n\nContainerCreating\n- Image pull issues \u2192 you\u2019ll see ErrImagePull/ImagePullBackOff; fix image name/tag/registry access; set imagePullSecrets.\n- Volume mount errors \u2192 missing Secret/ConfigMap/CSI driver; ensure referenced names exist and CSI is healthy.\n- CNI/sandbox errors (\u201cFailedCreatePodSandBox\u201d, \u201cfailed to setup network\u201d) \u2192 check CNI pods; kubectl get pods -n kube-system; fix CNI.\n- SecurityContext/capabilities denied by PSP/OPA/Kyverno \u2192 see \u201cforbidden\u201d in events; adjust policy or pod spec.\n\nImagePullBackOff / ErrImagePull\n- Wrong image or no auth \u2192 docker/podman pull same image to verify; add imagePullSecrets; avoid rate limits; use correct tag and registry FQDN.\n\nCrashLoopBackOff\n- App exits or probes failing.\n  - Check exit code/logs: kubectl logs \u2026 --previous\n  - Liveness/readiness failing \u2192 kubectl describe pod to see probe failures; adjust probes or app startup time.\n  - OOMKilled \u2192 see \u201cLast State: OOMKilled\u201d; increase memory limit, reduce usage.\n  - Crash during init \u2192 fix init container command/config.\n\nRunning but not Ready\n- Readiness probe failing \u2192 fix endpoint, path, port, thresholds, initialDelay.\n- Dependencies (DB, config) not reachable \u2192 check DNS/network policy; kubectl exec and curl/resolve.\n- Mounted config missing keys \u2192 verify ConfigMap/Secret contents.\n\nTerminating (won\u2019t go away)\n- Finalizers stuck \u2192 kubectl get pod <pod> -n <ns> -o json | jq .metadata.finalizers; clear only if you understand the controller impact.\n- PreStop hook hanging \u2192 see events; shorten or fix hook.\n- Node NotReady/kubelet down \u2192 check node; kubectl get nodes. As last resort: kubectl delete pod --grace-period=0 --force (may leak resources).\n- Volume detach stuck (CSI) \u2192 check events and CSI controller logs.\n\nEvicted\n- Node pressure (memory/disk/inodes) \u2192 kubectl describe node; free resources or tune eviction thresholds. Evicted pods must be recreated by their controller.\n\nInit:<state>\n- An init container is failing or waiting on something (e.g., volume, network). Check its logs and events.\n\n3) Cluster-wide quick scan\n- kubectl get events -A --sort-by=.lastTimestamp | tail -n 100\n- kubectl get nodes; kubectl describe node <node> (look for MemoryPressure/DiskPressure/NotReady)\n- Storage: kubectl get sc; kubectl get pvc,pv -A\n\nIf you share:\n- kubectl describe pod <name> -n <ns>\n- kubectl get pod <name> -n <ns> -o yaml (or at least the Status and Events)\n- kubectl get pvc -n <ns>\nI can pinpoint the exact cause and fix."
      },
      "gpt-5-nano_rag": {
        "input": {
          "question": "Why is my pod stuck?",
          "context": "kubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.\n\npod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nCheck the Service:\n\nkubectl get svc -l app.kubernetes.io/name=MyApp\n\nValidate that the Service receives a CLUSTER-IP address from the IPv6 address block along with an EXTERNAL-IP. You may then validate access to the service via the IP and port.\n\nNAME         TYPE           CLUSTER-IP            EXTERNAL-IP        PORT(S)        AGE\nmy-service   LoadBalancer   2001:db8:fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nThe kubelet manages the hosts file for each container of the Pod to prevent the container runtime from modifying the file after the containers have already been started. Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would then modify the /etc/hosts file after each container had started.\n\nCurrent Kubernetes can use a variety of container runtimes; even so, the kubelet manages the hosts file within each container so that the outcome is as intended regardless of which container runtime you use.\n\n{{< caution >}} Avoid making manual changes to the hosts file inside a container.\n\nIf you make manual changes to the hosts file, those changes are lost when the container exits. {{< /caution >}}",
          "context_docs": [
            {
              "content": "kubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.",
              "metadata": {
                "source": "k8s-docs/content/en/docs/tasks/network/validate-dual-stack.md",
                "_id": "495a2e90-c430-47d6-869c-3916c39c073d",
                "_collection_name": "k8s_docs"
              }
            },
            {
              "content": "pod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}",
              "metadata": {
                "source": "k8s-docs/content/en/docs/tasks/network/customize-hosts-file-for-pods.md",
                "_id": "2ebf09ce-41a3-4ce8-b80c-eeef5bd4f3e8",
                "_collection_name": "k8s_docs"
              }
            },
            {
              "content": "Check the Service:\n\nkubectl get svc -l app.kubernetes.io/name=MyApp\n\nValidate that the Service receives a CLUSTER-IP address from the IPv6 address block along with an EXTERNAL-IP. You may then validate access to the service via the IP and port.\n\nNAME         TYPE           CLUSTER-IP            EXTERNAL-IP        PORT(S)        AGE\nmy-service   LoadBalancer   2001:db8:fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s",
              "metadata": {
                "source": "k8s-docs/content/en/docs/tasks/network/validate-dual-stack.md",
                "_id": "4a7ac4d0-ae1a-4b74-9258-eabe6748d767",
                "_collection_name": "k8s_docs"
              }
            },
            {
              "content": "Why does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nThe kubelet manages the hosts file for each container of the Pod to prevent the container runtime from modifying the file after the containers have already been started. Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would then modify the /etc/hosts file after each container had started.\n\nCurrent Kubernetes can use a variety of container runtimes; even so, the kubelet manages the hosts file within each container so that the outcome is as intended regardless of which container runtime you use.\n\n{{< caution >}} Avoid making manual changes to the hosts file inside a container.\n\nIf you make manual changes to the hosts file, those changes are lost when the container exits. {{< /caution >}}",
              "metadata": {
                "source": "k8s-docs/content/en/docs/tasks/network/customize-hosts-file-for-pods.md",
                "_id": "2b6f9a06-03f1-4a08-8eeb-5ff5290d0825",
                "_collection_name": "k8s_docs"
              }
            }
          ],
          "prompt": "You are a Kubernetes expert. Use this context:\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.\n\npod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nCheck the Service:\n\nkubectl get svc -l app.kubernetes.io/name=MyApp\n\nValidate that the Service receives a CLUSTER-IP address from the IPv6 address block along with an EXTERNAL-IP. You may then validate access to the service via the IP and port.\n\nNAME         TYPE           CLUSTER-IP            EXTERNAL-IP        PORT(S)        AGE\nmy-service   LoadBalancer   2001:db8:fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nThe kubelet manages the hosts file for each container of the Pod to prevent the container runtime from modifying the file after the containers have already been started. Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would then modify the /etc/hosts file after each container had started.\n\nCurrent Kubernetes can use a variety of container runtimes; even so, the kubelet manages the hosts file within each container so that the outcome is as intended regardless of which container runtime you use.\n\n{{< caution >}} Avoid making manual changes to the hosts file inside a container.\n\nIf you make manual changes to the hosts file, those changes are lost when the container exits. {{< /caution >}}\n\nWhy is my pod stuck?",
          "system_prompt": "You are a Kubernetes expert. Use this context:\nkubectl exec -it pod01 -- set | grep MY_POD_IPS\n\nMY_POD_IPS=10.244.1.4,2001:db8::4\n\nThe Pod's IP addresses will also be written to /etc/hosts within a container. The following command executes a cat on /etc/hosts on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.\n\nkubectl exec -it pod01 -- cat /etc/hosts\n\n# Kubernetes-managed hosts file.\n127.0.0.1    localhost\n::1    localhost ip6-localhost ip6-loopback\nfe00::0    ip6-localnet\nfe00::0    ip6-mcastprefix\nfe00::1    ip6-allnodes\nfe00::2    ip6-allrouters\n10.244.1.4    pod01\n2001:db8::4    pod01\n\nValidate Services\n\nCreate the following Service that does not explicitly define .spec.ipFamilyPolicy. Kubernetes will assign a cluster IP for the Service from the first configured service-cluster-ip-range and set the .spec.ipFamilyPolicy to SingleStack.\n\n{{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}\n\nUse kubectl to view the YAML for the Service.\n\npod/hostaliases-pod created\n\nExamine a Pod's details to see its IPv4 address and its status:\n\nkubectl get pod --output=wide\n\nNAME                           READY     STATUS      RESTARTS   AGE       IP              NODE\nhostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0\n\nThe hosts file content looks like this:\n\nkubectl logs hostaliases-pod\n\n# Kubernetes-managed hosts file.\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nfe00::0 ip6-mcastprefix\nfe00::1 ip6-allnodes\nfe00::2 ip6-allrouters\n10.200.0.5  hostaliases-pod\n\n# Entries added by HostAliases.\n127.0.0.1   foo.local   bar.local\n10.1.2.3    foo.remote  bar.remote\n\nwith the additional entries specified at the bottom.\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nCheck the Service:\n\nkubectl get svc -l app.kubernetes.io/name=MyApp\n\nValidate that the Service receives a CLUSTER-IP address from the IPv6 address block along with an EXTERNAL-IP. You may then validate access to the service via the IP and port.\n\nNAME         TYPE           CLUSTER-IP            EXTERNAL-IP        PORT(S)        AGE\nmy-service   LoadBalancer   2001:db8:fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s\n\nWhy does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}\n\nThe kubelet manages the hosts file for each container of the Pod to prevent the container runtime from modifying the file after the containers have already been started. Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would then modify the /etc/hosts file after each container had started.\n\nCurrent Kubernetes can use a variety of container runtimes; even so, the kubelet manages the hosts file within each container so that the outcome is as intended regardless of which container runtime you use.\n\n{{< caution >}} Avoid making manual changes to the hosts file inside a container.\n\nIf you make manual changes to the hosts file, those changes are lost when the container exits. {{< /caution >}}",
          "human_prompt": "Why is my pod stuck?"
        },
        "response": "Hard to say without details. \u201cStuck\u201d can mean different pod phases. Please run these and share the outputs so we can pinpoint it:\n- kubectl get pod <name> -n <ns> -o wide\n- kubectl describe pod <name> -n <ns>\n- kubectl get events -n <ns> --sort-by=.lastTimestamp\n- kubectl logs <name> -n <ns> -c <container> --previous (if the container restarts)\n- If using volumes: kubectl get pvc -n <ns> && kubectl describe pvc <pvc>\n\nQuick triage by status:\n- Pending\n  - Unschedulable: node resource pressure or constraints. Look in describe events for \u201cInsufficient cpu/memory\u201d, nodeSelector/affinity mismatch, or taints (\u201chad taint {\u2026}\u201d). Fix requests/limits, relax constraints, or add tolerations.\n  - PVC not bound: events show \u201cpersistentvolumeclaim is not bound\u201d. Check StorageClass, PV availability, access modes.\n- ContainerCreating / PodInitializing\n  - CNI/networking: events like \u201cFailedCreatePodSandBox\u201d, \u201ccni failed to set up pod network\u201d. Check CNI pods in kube-system and node logs. In dual-stack clusters ensure your CNI is configured for dual-stack; a misconfigured CNI often leaves pods stuck here.\n    - kubectl get pods -n kube-system\n    - kubectl describe pod <name> (look for CNI/IPAM errors)\n  - Image pulls not started yet, or volume/secret/configmap mount errors (events: \u201csecret not found\u201d, \u201cconfigmap not found\u201d).\n- ErrImagePull / ImagePullBackOff\n  - Wrong image name/tag, private registry auth, or no egress/DNS. Try docker/podman pull from a node, set imagePullSecrets, or fix network/DNS.\n- CrashLoopBackOff\n  - App is starting then exiting. Get last logs with --previous, check liveness probes (too aggressive), config/env/secrets. Consider removing the liveness probe while debugging.\n- Running but NotReady\n  - Readiness probe failing, dependencies not reachable, or ports not listening. kubectl describe shows probe failures.\n- Terminating\n  - Finalizers or long shutdown. Check finalizers: kubectl get pod <name> -n <ns> -o jsonpath='{.metadata.finalizers}'. PreStop hooks and terminationGracePeriodSeconds can also delay deletion. Volume detach issues can also hang.\n- Unknown\n  - Node is NotReady or unreachable. kubectl get nodes; check kubelet and node health.\n\nDual-stack specifics (since many \u201cstuck\u201d cases are CNI-related):\n- Pods should get both IPv4 and IPv6 if the cluster and CNI are dual-stack capable. If events show IPAM/IPv6 errors, verify:\n  - Your CNI plugin supports and is configured for dual-stack.\n  - service-cluster-ip-range and cluster CIDRs are dual-stack, and the node/CNI has matching IP pools.\n- You can verify pod IPs inside the container:\n  - kubectl exec <pod> -- printenv MY_POD_IPS\n  - kubectl exec <pod> -- cat /etc/hosts\n- Note: kubelet manages /etc/hosts inside containers; manual edits won\u2019t persist. Use DNS or hostAliases in the Pod spec if you need custom entries.\n\nShare the outputs from describe, events, and logs and I\u2019ll pinpoint the cause and the exact fix."
      }
    }
  }
}